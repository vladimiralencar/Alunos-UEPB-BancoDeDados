{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-Multilayer Perceptron.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladimiralencar/Alunos-UEPB-BancoDeDados/blob/master/MLP/03_Multilayer_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "dYSBPXaDakzh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Multilayer Perceptron - Implementação mais simples"
      ]
    },
    {
      "metadata": {
        "id": "ChK_zQdTakzj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Implementação com TensorFlow "
      ]
    },
    {
      "metadata": {
        "id": "AMhdQilYakzw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "bec506fe-0017-438f-dae8-fef5c74ea04d"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "\n",
        "##########################\n",
        "### DATASET\n",
        "##########################\n",
        "filetitan = \"/media/datasets/DeepLearningI/Cap04/MNIST\"\n",
        "file = '02-Multilayer-Perceptron/MNIST'\n",
        "file = '.'\n",
        "mnist = input_data.read_data_sets(file, one_hot = True)\n",
        "\n",
        "\n",
        "##########################\n",
        "### Configurações\n",
        "##########################\n",
        "\n",
        "# Hiperparâmetros\n",
        "learning_rate = 0.1\n",
        "training_epochs = 30\n",
        "batch_size = 64\n",
        "\n",
        "# Arquitetura - parâmetros\n",
        "n_hidden_1 = 128\n",
        "n_hidden_2 = 256\n",
        "n_input = 784\n",
        "n_classes = 10\n",
        "\n",
        "\n",
        "##########################\n",
        "### Definição do Grafo\n",
        "##########################\n",
        "\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "\n",
        "    # Input data\n",
        "    tf_x = tf.placeholder(tf.float32, [None, n_input], name = 'features')\n",
        "    tf_y = tf.placeholder(tf.float32, [None, n_classes], name = 'targets')\n",
        "\n",
        "    # Multilayer perceptron\n",
        "    layer_1 = tf.layers.dense(tf_x, \n",
        "                              n_hidden_1, \n",
        "                              activation = tf.nn.relu, \n",
        "                              kernel_initializer = tf.truncated_normal_initializer(stddev = 0.1))\n",
        "    \n",
        "    layer_2 = tf.layers.dense(layer_1, \n",
        "                              n_hidden_2, \n",
        "                              activation = tf.nn.relu,\n",
        "                              kernel_initializer=tf.truncated_normal_initializer(stddev = 0.1))\n",
        "    \n",
        "    layer_3 = tf.layers.dense(layer_2, \n",
        "                              n_hidden_2, \n",
        "                              activation = tf.nn.relu,\n",
        "                              kernel_initializer=tf.truncated_normal_initializer(stddev = 0.1))\n",
        "    \n",
        "    \n",
        "    out_layer = tf.layers.dense(layer_3, \n",
        "                                n_classes, \n",
        "                                activation = None)\n",
        "\n",
        "    # Loss e optimizer\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=out_layer, labels=tf_y)\n",
        "    cost = tf.reduce_mean(loss, name='cost')\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "    train = optimizer.minimize(cost, name = 'train')\n",
        "\n",
        "    # Previsões\n",
        "    correct_prediction = tf.equal(tf.argmax(tf_y, 1), tf.argmax(out_layer, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = 'accuracy')\n",
        "\n",
        "\n",
        "##########################\n",
        "### Treinamento e Avaliação\n",
        "##########################\n",
        "    \n",
        "with tf.Session(graph=g) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch in range(training_epochs):\n",
        "        avg_cost = 0.\n",
        "        total_batch = mnist.train.num_examples // batch_size\n",
        "\n",
        "        for i in range(total_batch):\n",
        "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "            _, c = sess.run(['train', 'cost:0'], feed_dict = {'features:0': batch_x,\n",
        "                                                              'targets:0': batch_y})\n",
        "            avg_cost += c\n",
        "        \n",
        "        train_acc = sess.run('accuracy:0', feed_dict = {'features:0': mnist.train.images,\n",
        "                                                        'targets:0': mnist.train.labels})\n",
        "        \n",
        "        valid_acc = sess.run('accuracy:0', feed_dict = {'features:0': mnist.validation.images,\n",
        "                                                        'targets:0': mnist.validation.labels})  \n",
        "        \n",
        "        print(\"Epoch: %03d | AvgCost: %.3f\" % (epoch + 1, avg_cost / (i + 1)), end=\"\")\n",
        "        print(\" | Acurácia em Treino/Validação: %.3f/%.3f\" % (train_acc, valid_acc))\n",
        "        \n",
        "    test_acc = sess.run('accuracy:0', feed_dict = {'features:0': mnist.test.images,\n",
        "                                                   'targets:0': mnist.test.labels})\n",
        "    print('Acurácia em Teste: %.3f' % test_acc)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting ./train-images-idx3-ubyte.gz\n",
            "Extracting ./train-labels-idx1-ubyte.gz\n",
            "Extracting ./t10k-images-idx3-ubyte.gz\n",
            "Extracting ./t10k-labels-idx1-ubyte.gz\n",
            "Epoch: 001 | AvgCost: 0.329 | Acurácia em Treino/Validação: 0.956/0.957\n",
            "Epoch: 002 | AvgCost: 0.140 | Acurácia em Treino/Validação: 0.971/0.967\n",
            "Epoch: 003 | AvgCost: 0.100 | Acurácia em Treino/Validação: 0.980/0.973\n",
            "Epoch: 004 | AvgCost: 0.074 | Acurácia em Treino/Validação: 0.983/0.976\n",
            "Epoch: 005 | AvgCost: 0.059 | Acurácia em Treino/Validação: 0.989/0.977\n",
            "Epoch: 006 | AvgCost: 0.047 | Acurácia em Treino/Validação: 0.988/0.976\n",
            "Epoch: 007 | AvgCost: 0.038 | Acurácia em Treino/Validação: 0.991/0.975\n",
            "Epoch: 008 | AvgCost: 0.031 | Acurácia em Treino/Validação: 0.995/0.979\n",
            "Epoch: 009 | AvgCost: 0.024 | Acurácia em Treino/Validação: 0.989/0.976\n",
            "Epoch: 010 | AvgCost: 0.019 | Acurácia em Treino/Validação: 0.995/0.977\n",
            "Epoch: 011 | AvgCost: 0.015 | Acurácia em Treino/Validação: 0.997/0.978\n",
            "Epoch: 012 | AvgCost: 0.011 | Acurácia em Treino/Validação: 0.999/0.980\n",
            "Epoch: 013 | AvgCost: 0.009 | Acurácia em Treino/Validação: 0.997/0.978\n",
            "Epoch: 014 | AvgCost: 0.006 | Acurácia em Treino/Validação: 1.000/0.980\n",
            "Epoch: 015 | AvgCost: 0.004 | Acurácia em Treino/Validação: 1.000/0.981\n",
            "Epoch: 016 | AvgCost: 0.003 | Acurácia em Treino/Validação: 1.000/0.981\n",
            "Epoch: 017 | AvgCost: 0.002 | Acurácia em Treino/Validação: 1.000/0.981\n",
            "Epoch: 018 | AvgCost: 0.002 | Acurácia em Treino/Validação: 1.000/0.981\n",
            "Epoch: 019 | AvgCost: 0.001 | Acurácia em Treino/Validação: 1.000/0.982\n",
            "Epoch: 020 | AvgCost: 0.001 | Acurácia em Treino/Validação: 1.000/0.982\n",
            "Epoch: 021 | AvgCost: 0.001 | Acurácia em Treino/Validação: 1.000/0.982\n",
            "Epoch: 022 | AvgCost: 0.001 | Acurácia em Treino/Validação: 1.000/0.981\n",
            "Epoch: 023 | AvgCost: 0.001 | Acurácia em Treino/Validação: 1.000/0.982\n",
            "Epoch: 024 | AvgCost: 0.001 | Acurácia em Treino/Validação: 1.000/0.982\n",
            "Epoch: 025 | AvgCost: 0.001 | Acurácia em Treino/Validação: 1.000/0.982\n",
            "Epoch: 026 | AvgCost: 0.001 | Acurácia em Treino/Validação: 1.000/0.982\n",
            "Epoch: 027 | AvgCost: 0.001 | Acurácia em Treino/Validação: 1.000/0.982\n",
            "Epoch: 028 | AvgCost: 0.000 | Acurácia em Treino/Validação: 1.000/0.982\n",
            "Epoch: 029 | AvgCost: 0.000 | Acurácia em Treino/Validação: 1.000/0.982\n",
            "Epoch: 030 | AvgCost: 0.000 | Acurácia em Treino/Validação: 1.000/0.982\n",
            "Acurácia em Teste: 0.980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7JA-dl96akz1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Acurácia em Teste: 0.978 - (10 Epochs)\n",
        "0.981 (20 Epochs)\n",
        "Acurácia em Teste: 0.980 - (25 Epochs)\n",
        "Acurácia em Teste: 0.981 -    (30 Epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}