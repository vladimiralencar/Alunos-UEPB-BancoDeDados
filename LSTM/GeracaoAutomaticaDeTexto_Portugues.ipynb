{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GeracaoAutomaticaDeTexto-Portugues.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vladimiralencar/Alunos-UEPB-BancoDeDados/blob/master/LSTM/GeracaoAutomaticaDeTexto_Portugues.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "cyl7oi3xS739",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Geração Automática de Texto com LSTMs - Português"
      ]
    },
    {
      "metadata": {
        "id": "1nNr_MA9S73_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As Redes Neurais Recorrentes também podem ser usadas como modelos generativos. Isso significa que além de serem usadas como modelos preditivos (fazendo previsões), elas podem aprender as sequências de um problema e, em seguida, gerar sequências plausíveis inteiramente novas para o domínio do problema. Modelos Generativos como este são úteis não apenas para estudar o quão bem um modelo aprendeu um problema, mas para saber mais sobre o próprio domínio do problema. "
      ]
    },
    {
      "metadata": {
        "id": "DUjHNeirS74A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Uma boa forma de praticar a criação de texto, é usando livros clássicos, os quais já temos uma boa ideia sobre a história e que não estejamos violando nenhum direito de copyright. Muitos livros clássicos já não possuem mais restrição de uso e podem ser usados gratuitamente. Um bom lugar para encontrar esses livros é no site do Projeto Gutenberg. É de lá que usaremos o livro para o qual criaremos um modelo generativo: Alice no País das Maravilhas ou o nome em inglês Alice's Adventures in Wonderland. O arquivo txt do livro pode ser baixado aqui: https://www.gutenberg.org/ebooks/11. \n",
        "Este livro tem cerca de 3.300 linhas de texto. O cabeçalho e a marca de final de arquivo foram removidos, já que não são necessários para o que vamos fazer."
      ]
    },
    {
      "metadata": {
        "id": "W0ghRSivS74B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Vamos aprender as dependências entre os caracteres e as probabilidades condicionais de caracteres em sequências para que possamos gerar sequências totalmente novas e originais de caracteres. Esta é uma tarefa divertida e recomendo repetir essas experiências com outros livros do Projeto Gutenberg. Essas experiências não se limitam ao texto, você também pode experimentar com outros dados ASCII, como código fonte de linguagens de programação, documentos marcados em LaTeX, HTML ou Markdown e muito mais. \n",
        "\n",
        "Faremos aqui algo muito similar ao que foi feito pelo programador, que escreveu um novo livro de Game ofthrones: http://www.businessinsider.com/ai-just-wrote-the-next-book-of-game-of-thrones-for-us-2017-8"
      ]
    },
    {
      "metadata": {
        "id": "h5TO0IJmS74C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c8f48b8-ffce-46f0-b56e-063e84073f53"
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy\n",
        "import sys\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "pJKPbdY_S74K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "53dc0171-c871-46e7-9f5e-df0547ba68e5"
      },
      "cell_type": "code",
      "source": [
        "# Carregamos os dados e convertemos para lowercase \n",
        "# Estamos usando aqui arquivo texto no formato ASCII\n",
        "!wget https://raw.githubusercontent.com/vladimiralencar/DeepLearning-LANA/master/LSTM/data/O_Alienista.txt\n",
        "filename = \"O_Alienista.txt\" # livro de Machado de Assis\n",
        "raw_text = open(filename).read()\n",
        "raw_text = raw_text.lower()\n",
        "print(raw_text[:500])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-23 15:22:45--  https://raw.githubusercontent.com/vladimiralencar/DeepLearning-LANA/master/LSTM/data/O_Alienista.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102196 (100K) [text/plain]\n",
            "Saving to: ‘O_Alienista.txt.1’\n",
            "\n",
            "\rO_Alienista.txt.1     0%[                    ]       0  --.-KB/s               \rO_Alienista.txt.1   100%[===================>]  99.80K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-01-23 15:22:46 (2.57 MB/s) - ‘O_Alienista.txt.1’ saved [102196/102196]\n",
            "\n",
            "\n",
            "as crônicas da vila de itaguaí dizem que em tempos remotos vivera ali um certo médico, o dr.\n",
            "simão bacamarte, filho da nobreza da terra e o maior dos médicos do brasil, de portugal e das espanhas.\n",
            "estudara em coimbra e pádua. aos trinta e quatro anos regressou ao brasil, não podendo el-rei alcançar dele\n",
            "que ficasse em coimbra, regendo a universidade, ou em lisboa, expedindo os negócios da monarquia.\n",
            "—a ciência, disse ele a sua majestade, é o meu emprego único; itaguaí é o meu universo.\n",
            "dito iss\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I-KeSLMtS74O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora que o livro está carregado, devemos preparar os dados para modelagem. Não podemos modelar os caracteres diretamente, em vez disso, devemos converter os caracteres em números inteiros. Podemos fazer isso facilmente, criando um conjunto de todos os caracteres distintos do livro, então criando um mapa de cada caractere para um único inteiro."
      ]
    },
    {
      "metadata": {
        "id": "9rjKNmrWS74P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Criando o mapeamento caracter/inteiro\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MYVRURhJS74S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1139
        },
        "outputId": "ab33a223-2941-442d-975b-4240229a0028"
      },
      "cell_type": "code",
      "source": [
        "chars"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " '(',\n",
              " ')',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'x',\n",
              " 'z',\n",
              " '§',\n",
              " '°',\n",
              " 'º',\n",
              " 'à',\n",
              " 'á',\n",
              " 'â',\n",
              " 'ã',\n",
              " 'ç',\n",
              " 'é',\n",
              " 'ê',\n",
              " 'í',\n",
              " 'ò',\n",
              " 'ó',\n",
              " 'ô',\n",
              " 'õ',\n",
              " 'ú',\n",
              " 'ü',\n",
              " '—',\n",
              " '’',\n",
              " '“',\n",
              " '”']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "4MP8CeqES74X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1139
        },
        "outputId": "d033674a-62dc-4920-99a6-cae1eedbfa28"
      },
      "cell_type": "code",
      "source": [
        "char_to_int"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " '(': 4,\n",
              " ')': 5,\n",
              " ',': 6,\n",
              " '-': 7,\n",
              " '.': 8,\n",
              " '0': 9,\n",
              " '1': 10,\n",
              " '2': 11,\n",
              " '3': 12,\n",
              " '4': 13,\n",
              " '5': 14,\n",
              " '6': 15,\n",
              " '7': 16,\n",
              " '8': 17,\n",
              " '9': 18,\n",
              " ':': 19,\n",
              " ';': 20,\n",
              " '?': 21,\n",
              " 'a': 22,\n",
              " 'b': 23,\n",
              " 'c': 24,\n",
              " 'd': 25,\n",
              " 'e': 26,\n",
              " 'f': 27,\n",
              " 'g': 28,\n",
              " 'h': 29,\n",
              " 'i': 30,\n",
              " 'j': 31,\n",
              " 'l': 32,\n",
              " 'm': 33,\n",
              " 'n': 34,\n",
              " 'o': 35,\n",
              " 'p': 36,\n",
              " 'q': 37,\n",
              " 'r': 38,\n",
              " 's': 39,\n",
              " 't': 40,\n",
              " 'u': 41,\n",
              " 'v': 42,\n",
              " 'x': 43,\n",
              " 'z': 44,\n",
              " '§': 45,\n",
              " '°': 46,\n",
              " 'º': 47,\n",
              " 'à': 48,\n",
              " 'á': 49,\n",
              " 'â': 50,\n",
              " 'ã': 51,\n",
              " 'ç': 52,\n",
              " 'é': 53,\n",
              " 'ê': 54,\n",
              " 'í': 55,\n",
              " 'ò': 56,\n",
              " 'ó': 57,\n",
              " 'ô': 58,\n",
              " 'õ': 59,\n",
              " 'ú': 60,\n",
              " 'ü': 61,\n",
              " '—': 62,\n",
              " '’': 63,\n",
              " '“': 64,\n",
              " '”': 65}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "w2EjSE1xS74c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pode haver alguns caracteres que podemos remover para limpar mais o conjunto de dados que reduzirá o vocabulário e poderá melhorar o processo de modelagem. "
      ]
    },
    {
      "metadata": {
        "id": "FhS5n-snS74d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b3d77ed1-1210-444f-b2ca-87dce3965180"
      },
      "cell_type": "code",
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters: {:,d}\".format(n_chars))  \n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters: 99,080\n",
            "Total Vocab:  66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dXMg3KVXS74h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Podemos ver que o livro tem pouco mais de 99.000 caracteres e que quando convertidos para minúsculas, existem apenas 44 caracteres distintos no vocabulário para a rede aprender, muito mais do que os 26 no alfabeto. Agora, precisamos definir os dados de treinamento para a rede. Existe muita flexibilidade em como você escolhe dividir o texto e expô-lo a rede durante o treino. Aqui dividiremos o texto do livro em subsequências com um comprimento de 100 caracteres, um comprimento arbitrário. Poderíamos facilmente dividir os dados por sentenças e ajustar as sequências mais curtas e truncar as mais longas. Cada padrão de treinamento da rede é composto de 100 passos de tempo (time steps) de um caractere (X) seguido por um caracter de saída (y). Ao criar essas sequências, deslizamos esta janela ao longo de todo o livro um caracter de cada vez, permitindo que cada caracter tenha a chance de ser aprendido a partir dos 100 caracteres que o precederam (exceto os primeiros 100 caracteres, é claro). Por exemplo, se o comprimento da sequência é 5 (para simplificar), os dois primeiros padrões de treinamento seriam os seguintes:\n",
        "\n",
        "* Palavra: CHAPTER\n",
        "* CHAPT -> E\n",
        "* HAPTE -> R"
      ]
    },
    {
      "metadata": {
        "id": "bX_cIbVfS74j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc0d0de3-beaa-465d-e7ab-0bf4fa45e075"
      },
      "cell_type": "code",
      "source": [
        "# À medida que dividimos o livro em sequências, convertemos os caracteres em números inteiros usando nossa\n",
        "# tabela de pesquisa que preparamos anteriormente.\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "    seq_in = raw_text[i:i + seq_length]\n",
        "    seq_out = raw_text[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print (\"Total de Padrões: \", n_patterns)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total de Padrões:  98980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OT76IHpNS74n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Agora que preparamos nossos dados de treinamento, precisamos transformá-lo para que possamos usá-lo com o Keras. Primeiro, devemos transformar a lista de sequências de entrada na forma [amostras, passos de tempo, recursos] esperados por uma rede LSTM. Em seguida, precisamos redimensionar os números inteiros para o intervalo de 0 a 1 para tornar os padrões mais fáceis de aprender pela rede LSTM que usa a função de ativação sigmoide por padrão."
      ]
    },
    {
      "metadata": {
        "id": "4tUC1n9qS74o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reshape de X para [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "# Normalização\n",
        "X = X / float(n_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jVD_-gupS74s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finalmente, precisamos converter os padrões de saída (caracteres únicos convertidos em números inteiros) usando Hot-Encoding. Isto é para que possamos configurar a rede para prever a probabilidade de cada um dos 44 caracteres diferentes no vocabulário (uma representação mais fácil) em vez de tentar forçá-lo a prever com precisão o próximo caracter. Cada valor de y é convertido em um vetor com um comprimento 66, cheio de zeros, exceto com um 1 na coluna para a letra (inteiro) que o padrão representa. Por exemplo, quando a letra n (valor inteiro 30) tiver sido transformada usando One-Hot Encoding, vai se parecer com isso:\n",
        "\n",
        "[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
      ]
    },
    {
      "metadata": {
        "id": "MlcbxrukS74t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6b896913-0c48-420f-ca8a-2dd18dbee7f2"
      },
      "cell_type": "code",
      "source": [
        "# One-Hot Encoding da variável de saída\n",
        "y = np_utils.to_categorical(dataY)\n",
        "y[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "WZVdgHi1S74z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c55ded4-d38c-4f43-df99-e2d2b6777b3b"
      },
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(98980, 100, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "Q2n2mzySS744",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Modelo LSTM com duas camadas de Dropout com 20%\n",
        "# O tempo de treinamento é bem longo - \n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "azLX8d7VS74-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Não há conjunto de dados de teste. Estamos modelando todo o conjunto de dados de treinamento para aprender a probabilidade de cada caracter em uma sequência. Não estamos interessados nos mais preciso modelo do conjunto de dados de treinamento (Acurácia de Classificação). Este seria um modelo que prevê cada caracter no conjunto de dados de treinamento perfeitamente. Em vez disso, estamos interessados em uma generalização do conjunto de dados que minimiza a função de perda escolhida. Estamos buscando um equilíbrio entre generalização e\n",
        "overfitting."
      ]
    },
    {
      "metadata": {
        "id": "VXb4XIenS74_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define o checkpoint\n",
        "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z5c9W2iOS75L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fit do modelo"
      ]
    },
    {
      "metadata": {
        "id": "43bvjg_5S75N",
        "colab_type": "code",
        "colab": {},
        "outputId": "e9a6539a-bb76-4bf8-e222-9193da26c644"
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "dt = datetime.now()\n",
        "# model.fit(X, y, epochs = 20, batch_size = 128, callbacks = callbacks_list)\n",
        "#model.fit(X, y, epochs = 50, batch_size = 64, callbacks = callbacks_list)\n",
        "print(\"tempo de execução\", datetime.now() - dt)\n",
        "# tempo de execução 9:14:18 - 9 horas para treinar a rede LSTM"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.9924Epoch 00000: loss improved from inf to 2.99245, saving model to weights-improvement-00-2.9924.hdf5\n",
            "98980/98980 [==============================] - 653s - loss: 2.9924   \n",
            "Epoch 2/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.8144Epoch 00001: loss improved from 2.99245 to 2.81441, saving model to weights-improvement-01-2.8144.hdf5\n",
            "98980/98980 [==============================] - 653s - loss: 2.8144   \n",
            "Epoch 3/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.6979Epoch 00002: loss improved from 2.81441 to 2.69794, saving model to weights-improvement-02-2.6979.hdf5\n",
            "98980/98980 [==============================] - 653s - loss: 2.6979   \n",
            "Epoch 4/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.5828Epoch 00003: loss improved from 2.69794 to 2.58271, saving model to weights-improvement-03-2.5827.hdf5\n",
            "98980/98980 [==============================] - 655s - loss: 2.5827   \n",
            "Epoch 5/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.4803Epoch 00004: loss improved from 2.58271 to 2.48025, saving model to weights-improvement-04-2.4802.hdf5\n",
            "98980/98980 [==============================] - 747s - loss: 2.4802   \n",
            "Epoch 6/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.3981Epoch 00005: loss improved from 2.48025 to 2.39805, saving model to weights-improvement-05-2.3980.hdf5\n",
            "98980/98980 [==============================] - 675s - loss: 2.3980   \n",
            "Epoch 7/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.3279Epoch 00006: loss improved from 2.39805 to 2.32777, saving model to weights-improvement-06-2.3278.hdf5\n",
            "98980/98980 [==============================] - 808s - loss: 2.3278   \n",
            "Epoch 8/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.2672Epoch 00007: loss improved from 2.32777 to 2.26720, saving model to weights-improvement-07-2.2672.hdf5\n",
            "98980/98980 [==============================] - 862s - loss: 2.2672   \n",
            "Epoch 9/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.2152Epoch 00008: loss improved from 2.26720 to 2.21524, saving model to weights-improvement-08-2.2152.hdf5\n",
            "98980/98980 [==============================] - 650s - loss: 2.2152   \n",
            "Epoch 10/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.1670Epoch 00009: loss improved from 2.21524 to 2.16691, saving model to weights-improvement-09-2.1669.hdf5\n",
            "98980/98980 [==============================] - 650s - loss: 2.1669   \n",
            "Epoch 11/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.1194Epoch 00010: loss improved from 2.16691 to 2.11933, saving model to weights-improvement-10-2.1193.hdf5\n",
            "98980/98980 [==============================] - 651s - loss: 2.1193   \n",
            "Epoch 12/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.0817Epoch 00011: loss improved from 2.11933 to 2.08168, saving model to weights-improvement-11-2.0817.hdf5\n",
            "98980/98980 [==============================] - 657s - loss: 2.0817   \n",
            "Epoch 13/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.0446Epoch 00012: loss improved from 2.08168 to 2.04446, saving model to weights-improvement-12-2.0445.hdf5\n",
            "98980/98980 [==============================] - 654s - loss: 2.0445   \n",
            "Epoch 14/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.2534Epoch 00013: loss did not improve\n",
            "98980/98980 [==============================] - 656s - loss: 2.2568   \n",
            "Epoch 15/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 13.7604Epoch 00014: loss did not improve\n",
            "98980/98980 [==============================] - 653s - loss: 13.7562   \n",
            "Epoch 16/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 2.0058Epoch 00015: loss improved from 2.04446 to 2.00573, saving model to weights-improvement-15-2.0057.hdf5\n",
            "98980/98980 [==============================] - 655s - loss: 2.0057   \n",
            "Epoch 17/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.9505Epoch 00016: loss improved from 2.00573 to 1.95054, saving model to weights-improvement-16-1.9505.hdf5\n",
            "98980/98980 [==============================] - 652s - loss: 1.9505   \n",
            "Epoch 18/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.9189Epoch 00017: loss improved from 1.95054 to 1.91903, saving model to weights-improvement-17-1.9190.hdf5\n",
            "98980/98980 [==============================] - 655s - loss: 1.9190   \n",
            "Epoch 19/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.8906Epoch 00018: loss improved from 1.91903 to 1.89059, saving model to weights-improvement-18-1.8906.hdf5\n",
            "98980/98980 [==============================] - 656s - loss: 1.8906   \n",
            "Epoch 20/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.8657Epoch 00019: loss improved from 1.89059 to 1.86565, saving model to weights-improvement-19-1.8656.hdf5\n",
            "98980/98980 [==============================] - 651s - loss: 1.8656   \n",
            "Epoch 21/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.8412Epoch 00020: loss improved from 1.86565 to 1.84124, saving model to weights-improvement-20-1.8412.hdf5\n",
            "98980/98980 [==============================] - 675s - loss: 1.8412   \n",
            "Epoch 22/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.8182Epoch 00021: loss improved from 1.84124 to 1.81810, saving model to weights-improvement-21-1.8181.hdf5\n",
            "98980/98980 [==============================] - 669s - loss: 1.8181   \n",
            "Epoch 23/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.7940Epoch 00022: loss improved from 1.81810 to 1.79409, saving model to weights-improvement-22-1.7941.hdf5\n",
            "98980/98980 [==============================] - 654s - loss: 1.7941   \n",
            "Epoch 24/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.7738Epoch 00023: loss improved from 1.79409 to 1.77373, saving model to weights-improvement-23-1.7737.hdf5\n",
            "98980/98980 [==============================] - 657s - loss: 1.7737   \n",
            "Epoch 25/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.7562Epoch 00024: loss improved from 1.77373 to 1.75612, saving model to weights-improvement-24-1.7561.hdf5\n",
            "98980/98980 [==============================] - 676s - loss: 1.7561   \n",
            "Epoch 26/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.7358Epoch 00025: loss improved from 1.75612 to 1.73594, saving model to weights-improvement-25-1.7359.hdf5\n",
            "98980/98980 [==============================] - 656s - loss: 1.7359   \n",
            "Epoch 27/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.7206Epoch 00026: loss improved from 1.73594 to 1.72068, saving model to weights-improvement-26-1.7207.hdf5\n",
            "98980/98980 [==============================] - 657s - loss: 1.7207   \n",
            "Epoch 28/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.7019Epoch 00027: loss improved from 1.72068 to 1.70200, saving model to weights-improvement-27-1.7020.hdf5\n",
            "98980/98980 [==============================] - 653s - loss: 1.7020   \n",
            "Epoch 29/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.6831Epoch 00028: loss improved from 1.70200 to 1.68299, saving model to weights-improvement-28-1.6830.hdf5\n",
            "98980/98980 [==============================] - 651s - loss: 1.6830   \n",
            "Epoch 30/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.6714Epoch 00029: loss improved from 1.68299 to 1.67137, saving model to weights-improvement-29-1.6714.hdf5\n",
            "98980/98980 [==============================] - 651s - loss: 1.6714   \n",
            "Epoch 31/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.6539Epoch 00030: loss improved from 1.67137 to 1.65381, saving model to weights-improvement-30-1.6538.hdf5\n",
            "98980/98980 [==============================] - 652s - loss: 1.6538   \n",
            "Epoch 32/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.6425Epoch 00031: loss improved from 1.65381 to 1.64250, saving model to weights-improvement-31-1.6425.hdf5\n",
            "98980/98980 [==============================] - 652s - loss: 1.6425   \n",
            "Epoch 33/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.6258Epoch 00032: loss improved from 1.64250 to 1.62604, saving model to weights-improvement-32-1.6260.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "98980/98980 [==============================] - 651s - loss: 1.6260   \n",
            "Epoch 34/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.6108Epoch 00033: loss improved from 1.62604 to 1.61078, saving model to weights-improvement-33-1.6108.hdf5\n",
            "98980/98980 [==============================] - 651s - loss: 1.6108   \n",
            "Epoch 35/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.6012Epoch 00034: loss improved from 1.61078 to 1.60121, saving model to weights-improvement-34-1.6012.hdf5\n",
            "98980/98980 [==============================] - 653s - loss: 1.6012   \n",
            "Epoch 36/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5887Epoch 00035: loss improved from 1.60121 to 1.58865, saving model to weights-improvement-35-1.5887.hdf5\n",
            "98980/98980 [==============================] - 656s - loss: 1.5887   \n",
            "Epoch 37/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5806Epoch 00036: loss improved from 1.58865 to 1.58049, saving model to weights-improvement-36-1.5805.hdf5\n",
            "98980/98980 [==============================] - 655s - loss: 1.5805   \n",
            "Epoch 38/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5692Epoch 00037: loss improved from 1.58049 to 1.56930, saving model to weights-improvement-37-1.5693.hdf5\n",
            "98980/98980 [==============================] - 662s - loss: 1.5693   \n",
            "Epoch 39/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5594Epoch 00038: loss improved from 1.56930 to 1.55936, saving model to weights-improvement-38-1.5594.hdf5\n",
            "98980/98980 [==============================] - 657s - loss: 1.5594   \n",
            "Epoch 40/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5489Epoch 00039: loss improved from 1.55936 to 1.54879, saving model to weights-improvement-39-1.5488.hdf5\n",
            "98980/98980 [==============================] - 652s - loss: 1.5488   \n",
            "Epoch 41/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5393Epoch 00040: loss improved from 1.54879 to 1.53948, saving model to weights-improvement-40-1.5395.hdf5\n",
            "98980/98980 [==============================] - 651s - loss: 1.5395   \n",
            "Epoch 42/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5283Epoch 00041: loss improved from 1.53948 to 1.52834, saving model to weights-improvement-41-1.5283.hdf5\n",
            "98980/98980 [==============================] - 652s - loss: 1.5283   \n",
            "Epoch 43/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5233Epoch 00042: loss improved from 1.52834 to 1.52339, saving model to weights-improvement-42-1.5234.hdf5\n",
            "98980/98980 [==============================] - 656s - loss: 1.5234   \n",
            "Epoch 44/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5112Epoch 00043: loss improved from 1.52339 to 1.51134, saving model to weights-improvement-43-1.5113.hdf5\n",
            "98980/98980 [==============================] - 657s - loss: 1.5113   \n",
            "Epoch 45/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5090Epoch 00044: loss improved from 1.51134 to 1.50899, saving model to weights-improvement-44-1.5090.hdf5\n",
            "98980/98980 [==============================] - 657s - loss: 1.5090   \n",
            "Epoch 46/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.5010Epoch 00045: loss improved from 1.50899 to 1.50104, saving model to weights-improvement-45-1.5010.hdf5\n",
            "98980/98980 [==============================] - 658s - loss: 1.5010   \n",
            "Epoch 47/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.4886Epoch 00046: loss improved from 1.50104 to 1.48879, saving model to weights-improvement-46-1.4888.hdf5\n",
            "98980/98980 [==============================] - 653s - loss: 1.4888   \n",
            "Epoch 48/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.4803Epoch 00047: loss improved from 1.48879 to 1.48016, saving model to weights-improvement-47-1.4802.hdf5\n",
            "98980/98980 [==============================] - 653s - loss: 1.4802   \n",
            "Epoch 49/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.4750Epoch 00048: loss improved from 1.48016 to 1.47499, saving model to weights-improvement-48-1.4750.hdf5\n",
            "98980/98980 [==============================] - 655s - loss: 1.4750   \n",
            "Epoch 50/50\n",
            "98944/98980 [============================>.] - ETA: 0s - loss: 1.4807Epoch 00049: loss did not improve\n",
            "98980/98980 [==============================] - 657s - loss: 1.4806   \n",
            "tempo de execução 9:14:18.978356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rpeRRwaFS75U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Continuar treinando o modelo a partir do checkpoint"
      ]
    },
    {
      "metadata": {
        "id": "MPJ1v1YaS75V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "7e28ab86-e299-4356-ea3d-7647e5f375fb"
      },
      "cell_type": "code",
      "source": [
        "# Continuar treinando o modelo a partir do checkpoint\n",
        "!mkdir weights\n",
        "!wget https://raw.githubusercontent.com/vladimiralencar/DeepLearning-LANA/master/LSTM/data/weights-improvement-08-1.3758.hdf5-Portuguese\n",
        "!mv w*.* weights\n",
        "# Carrega os melhores pesos da rede e compila o modelo\n",
        "filename = 'weights/' +   \"weights-improvement-08-1.3758.hdf5-Portuguese\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
        "\n",
        "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5-Portuguese\"\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "from datetime import datetime\n",
        "dt = datetime.now()\n",
        "# fit the model\n",
        "#model.fit(X, y, epochs = 1, batch_size = 64, callbacks = callbacks_list)\n",
        "print(\"tempo de execução\", datetime.now() - dt)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘weights’: File exists\n",
            "--2019-01-23 16:11:26--  https://raw.githubusercontent.com/vladimiralencar/DeepLearning-LANA/master/LSTM/data/weights-improvement-08-1.3758.hdf5-Portuguese\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9704880 (9.3M) [application/octet-stream]\n",
            "Saving to: ‘weights-improvement-08-1.3758.hdf5-Portuguese’\n",
            "\n",
            "weights-improvement 100%[===================>]   9.25M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-01-23 16:11:27 (62.9 MB/s) - ‘weights-improvement-08-1.3758.hdf5-Portuguese’ saved [9704880/9704880]\n",
            "\n",
            "tempo de execução 0:00:00.000042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gVuW7UNXS75c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ddb83603-f6d2-4411-9e3f-50f5de48e6e3"
      },
      "cell_type": "code",
      "source": [
        "# tempo de execução 9:14:18 - 50 epoch\n",
        "# tempo de execução 1:50:49 - 10 epoch\n",
        "#!ls w*.-Portuguese\n",
        "!ls  weights"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights-improvement-08-1.3758.hdf5-Portuguese\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V9MnCc7MS75k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Depois de executar o fit, você deve ter uma série de arquivos de checkpoint no mesmo diretório onde está este Jupyter Notebook. Você pode excluí-los todos exceto aquele com o menor valor de perda. Por exemplo, neste caso, o arquivo weights-improvement-19-1.9119.hdf5 será usado. Ele contém os melhores valores de peso."
      ]
    },
    {
      "metadata": {
        "id": "R1qjzs_lS75l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Carrega os melhores pesos da rede e compila o modelo\n",
        "filename = \"weights/\" + \"weights-improvement-08-1.3758.hdf5-Portuguese\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e_p7kpcnS75n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r0m3VnU6S75q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text():\n",
        "    # Obtém um random seed\n",
        "    start = numpy.random.randint(0, len(dataX)-1)\n",
        "\n",
        "    # Inicia a geração de texto de um ponto qualquer, definido pelo random seed \"start\"\n",
        "    pattern = dataX[start]\n",
        "    print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "\n",
        "    # Gerando caracteres\n",
        "    for i in range(1000):\n",
        "        x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "        x = x / float(n_vocab)\n",
        "        prediction = model.predict(x, verbose=0)\n",
        "        index = numpy.argmax(prediction)\n",
        "        result = int_to_char[index]\n",
        "        seq_in = [int_to_char[value] for value in pattern]\n",
        "        sys.stdout.write(result)\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "    print('\\n=======================================================================================================\\n')\n",
        "\n",
        "    #print (\"\\nConcluído.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6z6DdnOAS75s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f175648c-2dfc-4327-a137-6457cf3c6a4b"
      },
      "cell_type": "code",
      "source": [
        "generate_text()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\" ista roçou a vaga sombra de uma intenção de riso, em que o\n",
            "desdém vinha casado à comiseração; mas ne \"\n",
            "nhuma pue ele asrimu a cincoiar de consesser e dessigar o donse mate mutros de dessrigu outros eram mortos eran tontades do povo eo pue a câmara recolhido ao barbeiro, a casa verde é um dos pontos para o porso mais belegiro e a casa verde é um cochuar de casa esi três o governo não podia des a coreldn que o alienista não o alienista foi o pue prdia ser ee uma peruersiria do alienista eos almdossos de poder a sua majestade. a puem entiruava a casa verde e de terris lais esa uma coisa mais de manhão com a ele, não a de janhia, e alnda o alienista puoiaio que vossa renhora e dessibado a putra de ser emrredo de casa espicado e desiguarses e conseger ao povo eoses de mongems, e deserninhar a parse a casa verde é um col de longin e desebrar aos outros, e a alia do res de janeiro. sem de uma peruecaram as alianiste. o povo ene nais a meio destribada d de todas as alianistos de simão bacamarte que o alienista fosse con erande a almga e aligos dos outros, o alienista não o alienista foi o pue p\n",
            "=======================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BzJlNEAQS75v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "f9fa64c1-07c3-4c47-dc4d-4b4990d10945"
      },
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    generate_text()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\" . simão\n",
            "bacamarte não o contrariou; disse, porém, aos circunstantes que o terror também é pai da lou \"\n",
            "cura, e aleun tenpo de cois mindos com a esc. aligade do alienista que o alienista contintava a casa verde e de terris erandes consra e almga e a responça de itaguaí e aastanda para o daso de dasa verde a sua majestade. um dos prineiras, a casa verde é um conterso de dezs e a de uma uenhas de um cor das cortes e assimentras e castiga de sua majestade, a puem enterdeitamdo o alienista que o alienista fosse con o alienista fosse continto e alma a dessrra da rerponta de itaguaí e asosria e a dessiga de que o alienista eosse continto e alma aodaça a casa verde, o alienista ficou a puem e de um cos mais consrigades mais aluitres de dois monensos da casa verde e de porta movícia de sua majestade. a puem incomtinte a esta dxperiência eo alienista que o alienista fosse con erande a almga e aligos dos outros, o alienista não o alienista foi o pue prdie fogarda do inuerno com a responta de sua majestade. um dos prineiras, a casa verde é um dos pontos para o ponferoo que o alienista fosse con o a\n",
            "=======================================================================================================\n",
            "\n",
            "\" ora, todo esse trabalho levava-lhe o melhor e o mais do tempo. mal dormia e mal comia; e, ainda\n",
            "come \"\n",
            "ltar o conceito de um alienida de sereridade do alienista que o alienista fosse con erande a almga e aligos dos outros, o alienista não o alienista foi o pue prdie fogarda do inuerno com a responta de sua majestade. um dos prineiras, a casa verde é um dos pontos para o ponferoo que o alienista fosse con o alienista fosse continto e alma a dessrra da rerponta de itaguaí e asosria e a dessiga de que o alienista eosse continto e alma aodaça a casa verde, o alienista ficou a puem e de um cos mais consrigades mais aluitres de dois monensos da casa verde e de porta movícia de sua majestade. a puem incomtinte a esta dxperiência eo alienista que o alienista fosse con erande a almga e aligos dos outros, o alienista não o alienista foi o pue prdie fogarda do inuerno com a responta de sua majestade. um dos prineiras, a casa verde é um dos pontos para o ponferoo que o alienista fosse con o alienista fosse continto e alma a dessrra da rerponta de itaguaí e asosria e a dessiga de que o alienista eos\n",
            "=======================================================================================================\n",
            "\n",
            "\" nal do § 4º, uma frase cheia de experiências futuras.\n",
            "capítulo xii - o final do § 4º.\n",
            "apagaram-se as \"\n",
            " selar de sua majestade. um dos prineiras, a casa verde é um dos pontos para o ponferoo que o alienista fosse con o alienista fosse continto e alma a dessrra da rerponta de itaguaí e asosria e a dessiga de que o alienista eosse continto e alma aodaça a casa verde, o alienista ficou a puem e de um cos mais consrigades mais aluitres de dois monensos da casa verde e de porta movícia de sua majestade. a puem incomtinte a esta dxperiência eo alienista que o alienista fosse con erande a almga e aligos dos outros, o alienista não o alienista foi o pue prdie fogarda do inuerno com a responta de sua majestade. um dos prineiras, a casa verde é um dos pontos para o ponferoo que o alienista fosse con o alienista fosse continto e alma a dessrra da rerponta de itaguaí e asosria e a dessiga de que o alienista eosse continto e alma aodaça a casa verde, o alienista ficou a puem e de um cos mais consrigades mais aluitres de dois monensos da casa verde e de porta movícia de sua majestade. a puem incomtin\n",
            "=======================================================================================================\n",
            "\n",
            "\" a rebeldia à medicação; mas, não sendo escritor (mal\n",
            "sabia assinar o nome), não se lhe podia aplicar \"\n",
            " o territo de dotue e de uma lateram de sodas a rua d des aoris de manhã,—aio oa rua u blienista dos orincirais da casa verde e de terris erandes consra e almda e aosas de desco a outro do boi da manta de ten oesmo a intenção de itaguaí, e alium tempo ce portura e deserninar a casa verde e de terris erandes consra e almda e aosas de desco a outro da casa verde e de terris era a imienta de que ele certoria de cinco derres e a das de uma penhura de que ele virera a casa verde, sem desenual de cinrüsiia que o alienista fosse con o alienista fosse continto e alma a dessrra da rerponta de itaguaí e asosria e a dessiga de que o alienista eosse continto e alma aodaça a casa verde, o alienista ficou a puem e de um cos mais consrigades mais aluitres de dois monensos da casa verde e de porta movícia de sua majestade. a puem incomtinte a esta dxperiência eo alienista que o alienista fosse con erande a almga e aligos dos outros, o alienista não o alienista foi o pue prdie fogarda do inuerno com a \n",
            "=======================================================================================================\n",
            "\n",
            "\" bro, com intimidade, davam-lhe piparotes no nariz,\n",
            "diziam-lhe pulhas. e o costa sempre lhano, risonh \"\n",
            "a de uma eeusa do blienista. e dizia ele ne diania, do que to dos parsis, desee que era um portar a era uma coisa mais de manhão, e apiso que não fra um dos pais cons pue ele mão conticendo a casa verde e de terris landos de cinco a esc. a alegria dos pue trdzente uma coisa mais de manhão cos dras sentazes contra e alma de casa espura do que a câmara lhe dar ums doises e o alienista que o alienista fosse con erande a almga e aligos dos outros, o alienista não o alienista foi o pue prdie fogarda do inuerno com a responta de sua majestade. um dos prineiras, a casa verde é um dos pontos para o ponferoo que o alienista fosse con o alienista fosse continto e alma a dessrra da rerponta de itaguaí e asosria e a dessiga de que o alienista eosse continto e alma aodaça a casa verde, o alienista ficou a puem e de um cos mais consrigades mais aluitres de dois monensos da casa verde e de porta movícia de sua majestade. a puem incomtinte a esta dxperiência eo alienista que o alienista fosse con eran\n",
            "=======================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VolGvfkkS75y",
        "colab_type": "code",
        "colab": {},
        "outputId": "edad2ae6-0dc4-4884-8d1b-06acf63888ec"
      },
      "cell_type": "code",
      "source": [
        "generate_text()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"  e que ele não podia deixar na rua um\n",
            "mentecapto. a última pessoa que intercedeu por ele (porque dep \"\n",
            "ois de casa a mais pidenda de um alcare de sereltou a casa verde era uma casa aorica, e a câmara lhe dasa de alguma casa verde. — a pazão do boticário. e dizer que era um cos damar, e a dâmara lhe deste lajs de eizer que o alienista fosse conceuso do alienista por angar comses a esperiar dos peus enteresses de ser emtregar com a alna, o perso de que a câmara lhe dasa de ser eme a casa verde era uma casa aoricado, mas acmanan-lhe o alienista a alguns senpos, a casa verde era uma casa aoricado, mas acmanan-lhe o alienista a alguns senpos, a casa verde era uma casa aoricado, mas acmanan-lhe o alienista a alguns senpos, a casa verde era uma casa aoricado, mas acmanan-lhe o alienista a alguns senpos, a casa verde era uma casa aoricado, mas acmanan-lhe o alienista a alguns senpos, a casa verde era uma casa aoricado, mas acmanan-lhe o alienista a alguns senpos, a casa verde era uma casa aoricado, mas acmanan-lhe o alienista a alguns senpos, a casa verde era uma casa aoricado, mas acmanan-lhe \n",
            "=======================================================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "49LWeLluS751",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}